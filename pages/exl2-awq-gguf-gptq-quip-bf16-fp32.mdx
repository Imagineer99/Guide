# **Guide to Model Formats for Artificial Intelligence**

*Model formats* refer to the way data is stored and processed by machine learning models. Different model formats have different advantages and trade-offs when it comes to performance, accuracy, and size. In this guide, we will explore some popular model formats used in artificial intelligence (AI) today.

## Floating Point 32 (fp32)

Floating point 32 (fp32) is a widely used model format in deep learning. It uses 32 bits to represent each number, providing high precision and dynamic range. However, its large memory footprint makes it less efficient for training and deploying large models.

## Half Precision (fp16)

Half precision (fp16) is a compact model format that uses only 16 bits per number. This reduces memory usage and improves throughput, making it ideal for training and deploying large models. However, it has lower precision than fp32, which may affect model accuracy.

## Brain Float 16 (bf16)

Brain float 16 (bf16) is a new model format developed by Google. It combines the benefits of both fp32 and fp16, offering higher precision than fp16 while maintaining the same memory usage. Bf16 is currently supported by TensorFlow and other popular deep learning frameworks.

## Activation Width Quantization (awq)

Activation width quantization (awq) is a technique for reducing the memory usage of neural networks. It involves quantizing the activation functions of neurons to lower bitwidths, such as INT8 or INT4. This reduces memory usage without significantly affecting model accuracy.

## Generalized Gradient Unfolding Format (gguf)

Generalized gradient unfolding format (gguf) is a sparse model format designed for training and deploying large models with billions of parameters. It represents gradients as dense vectors and weights as sparse tensors, allowing for more efficient computation and storage.

## Exponential Linear Unit (exl2)

Exponential linear unit (exl2) is a non-linear activation function used in deep learning. It has been shown to improve the convergence speed and generalization ability of neural networks. Exl2 models typically use fp32 or fp16 model formats.

## Quipper (quip)

Quipper (quip) is a quantum programming language and compiler developed by University of Oxford. It allows users to write scalable quantum algorithms and compile them to various quantum hardware platforms.

## GPTQ

GPTQ is a tool for pruning transformer models based on the GLU activation function. It reduces the model size dramatically while preserving the model quality well.

## Mewtwo

Mewtwo is a lightweight, portable, and extensible deep learning library written in C++. It supports multiple backends including CUDA, Metal, DirectX, Vulkan, and OpenCL.